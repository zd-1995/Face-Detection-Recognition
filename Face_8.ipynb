{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ullg3XjJVgiv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1H3Zf2NpVUf"
      },
      "outputs": [],
      "source": [
        "!pip install MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwpCJfRlocTA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import requests\n",
        "from io import BytesIO\n",
        "# extract and plot each detected face in a photograph\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle,Circle\n",
        "from mtcnn import MTCNN\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder,Normalizer\n",
        "from sklearn.svm import SVC\n",
        "from random import choice\n",
        "from PIL import Image as Img\n",
        "from numpy import savez_compressed,asarray,load,expand_dims\n",
        "from keras.models import load_model\n",
        "# from keras_facenet import FaceNet\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SucxddsPhOmj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx_16lnMqjvF"
      },
      "outputs": [],
      "source": [
        "# class Data_Using:\n",
        "\n",
        "def unzip(zip_dir,out_dir):\n",
        "    path_folder = out_dir\n",
        "\n",
        "    zip_file_path = f'{zip_dir}'\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(path_folder)\n",
        "\n",
        "\n",
        "def load_dataset(directory):\n",
        "  \t# enumerate folders, on per class\n",
        "  images = []\n",
        "  path_d = []\n",
        "  allpath = []\n",
        "  y = []\n",
        "  for subdir in os.listdir(directory):\n",
        "\n",
        "    # print(y)\n",
        "       \t# enumerate files\n",
        "    for filename in os.listdir(directory+subdir+'/'):\n",
        "      path = directory + subdir + '/' + filename\n",
        "          #labels\n",
        "      y.append(subdir)\n",
        "      path_d.append(path)\n",
        "      # print(path_d)\n",
        "  # allpath.extend(path_d)\n",
        "\n",
        "  print(path_d)\n",
        "\t  \t# store\n",
        "    # images.append(path_d)\n",
        "  return path_d , y\n",
        "\n",
        "\n",
        "def load_data_image(path_image):\n",
        "    img = cv2.imread(path_image)\n",
        "    # img = cv2.cvtColor(img,cv2.COLOR)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_data_video(path_video):\n",
        "\n",
        "    frame_list = []\n",
        "    # Load the video content using OpenCV\n",
        "    cap = cv2.VideoCapture(path_video)\n",
        "    # Loop through the frames and display them\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Display the frame\n",
        "        # cv2_imshow( frame)\n",
        "\n",
        "        frame_list.append(frame)\n",
        "\n",
        "        # Press 'q' to exit the loop\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    # Release the video capture object and close all windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    frame_list = np.array(frame_list)\n",
        "    return frame_list\n",
        "\n",
        "\n",
        "\n",
        "def load_data_system_camera():\n",
        "    # define a video capture object\n",
        "    vid = cv2.VideoCapture(0)\n",
        "    while(True):\n",
        "        # Capture the video frame\n",
        "        ret, frame = vid.read()\n",
        "        # Display the resulting frame\n",
        "        cv2.imshow('frame', frame)\n",
        "        cv2.imwrite('video.mp4',frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    # After the loop release the cap object\n",
        "    vid.release()\n",
        "    # Destroy all the windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK8LKMVIMZqv"
      },
      "outputs": [],
      "source": [
        "from functools import wraps\n",
        "import sys\n",
        "import io\n",
        "def capture_output(func):\n",
        "    \"\"\"Wrapper to capture print output.\"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        old_stdout = sys.stdout\n",
        "        new_stdout = io.StringIO()\n",
        "        sys.stdout = new_stdout\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mokJz5IlSoS6"
      },
      "outputs": [],
      "source": [
        "class Face_Detection_Recognition_Video:\n",
        "\n",
        "    def __init__(self):\n",
        "      # super.__init__(self)\n",
        "      self.face_number = 0\n",
        "      self.folder_number = 0\n",
        "\n",
        "    def preprocess(self,image):\n",
        "      #normalize\n",
        "        pixels = image.astype('float32')\n",
        "        pixels /= 255.0\n",
        "        return pixels\n",
        "\n",
        "      # draw each face separately\n",
        "    def mtcnn_model(self,data, result_list):\n",
        "        # get the context for drawing boxes\n",
        "        ax = pyplot.gca()\n",
        "        for result in result_list:\n",
        "          # get coordinates\n",
        "          x, y, width, height = result['box']\n",
        "          # create the shape\n",
        "          rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
        "          # draw the box\n",
        "          ax.add_patch(rect)\n",
        "          # draw the dots\n",
        "          for key, value in result['keypoints'].items():\n",
        "          # create and draw dot\n",
        "            dot = Circle(value, radius=2, color='red')\n",
        "            ax.add_patch(dot)\n",
        "        # pyplot.show()\n",
        "        crop_face = []\n",
        "        for i in range(len(result_list)):\n",
        "          # get coordinates\n",
        "          x1, y1, width, height = result_list[i]['box']\n",
        "          x2, y2 = x1 + width, y1 + height\n",
        "          # define subplot\n",
        "          pyplot.subplot(1, len(result_list), i+1)\n",
        "          pyplot.axis('off')\n",
        "          # plot face\n",
        "          Face = data[y1:y2, x1:x2]\n",
        "          # resize pixels to the model size\n",
        "          Face = Img.fromarray(Face)\n",
        "          Face = Face.convert('RGB')\n",
        "          Face = Face.resize((160,160))\n",
        "          Face = asarray(Face)#,dtype=np.float32)\n",
        "          crop_face.append(Face)\n",
        "\n",
        "        return crop_face\n",
        "\n",
        "\n",
        "    def get_face(self,path):\n",
        "        pixels = path\n",
        "        pixels = asarray(pixels)\n",
        "        detector = MTCNN()\n",
        "        # detect faces in the image\n",
        "        w_detect = capture_output( detector.detect_faces)\n",
        "        faces_pos = w_detect(pixels)\n",
        "        Faces = self.mtcnn_model(pixels, faces_pos)\n",
        "\n",
        "        return Faces , faces_pos\n",
        "\n",
        "\n",
        "    def model_CNN(self):\n",
        "      model_cnn = models.Sequential([\n",
        "      layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      # layers.Dropout(0.2),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      # layers.Dropout(0.2),\n",
        "      layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(512, activation='relu'),\n",
        "      layers.Dropout(0.2),  # Dropout for regularization\n",
        "      layers.Dense(128, activation='relu'),\n",
        "      layers.Dense(6, activation='softmax')  # num_classes is the number of identities\n",
        "      ])\n",
        "\n",
        "      return model_cnn\n",
        "\n",
        "    def train_cnn_model(self,trainx,trainy,testx, testy,E_number):\n",
        "      self.model = self.model_CNN()\n",
        "      self.model.summary()\n",
        "      self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "      self.model.fit(trainx, trainy, epochs= E_number)\n",
        "      test_loss, test_acc = self.model.evaluate(testx, testy)\n",
        "      return self.model , test_loss, test_acc\n",
        "\n",
        "\n",
        "    def recognition_model(self,image,model):\n",
        "      random_face = np.array(image)\n",
        "      random_face = random_face.reshape(1,160,160,3)\n",
        "      w_predict = capture_output(self.model.predict)\n",
        "      pred = w_predict(random_face)\n",
        "      # random_face_name = out_encoder.inverse_transform([random_face_class])\n",
        "      out_encoder = LabelEncoder()\n",
        "      out_encoder.fit(trainy)\n",
        "      class_index = tf.argmax(pred, axis=1)\n",
        "      # confidence = tf.reduce_max(pred[0][random_face_class])\n",
        "      pred_name = out_encoder.inverse_transform(class_index)\n",
        "      # print('Predicted: %s' % pred_name)\n",
        "      # print('Expected: %s' % random_face_name[0])\n",
        "      # print(f\"Predicted class: {class_index}, Confidence: {confidence * 100:.2f}%\")\n",
        "      # pyplot.figure()\n",
        "      # pyplot.imshow(random_face.reshape(160,160,3))\n",
        "      # pyplot.show()\n",
        "      return pred_name\n",
        "\n",
        "    def save_faces(self,data_face):\n",
        "      folder_name = 'Faces'+f'{self.folder_number}'\n",
        "      if not os.path.isdir(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "      cv2.imwrite(folder_name + '/' + f'{self.face_number}.jpg',data_face)\n",
        "      self.face_number += 1\n",
        "\n",
        "    def face_counting(self,):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO6ub9w-QK8o"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Facenet/Photos.zip'\n",
        "output = '/content/photos'\n",
        "data = unzip(path,output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY2grl0J6dCz"
      },
      "outputs": [],
      "source": [
        "!rm -R /content/photos/__MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT1kpS3Amx-P"
      },
      "outputs": [],
      "source": [
        "!find . -name \"*.DS_Store\" -type f -delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9uZOmYoVMMp",
        "outputId": "8fc369cc-e3fd-4c10-8baa-440210593f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/photos/train/zahra_dabiri/zahra_dabiri': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -R /content/photos/train/zahra_dabiri/zahra_dabiri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8MxpXbPMdzD"
      },
      "outputs": [],
      "source": [
        "%cp -av /content/drive/MyDrive/Facenet/zahra_dabiri /content/photos/train/zahra_dabiri\n",
        "%cp -av /content/drive/MyDrive/Facenet/zahra_dabiri_val /content/photos/val/zahra_dabiri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFYuXgmBAmG7"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "\n",
        "def faces_dataset(directory):\n",
        "  X = list()\n",
        "  newlabels = list()\n",
        "  files , labels = load_dataset(directory)\n",
        "  print(len(files),len(labels))\n",
        "  for fil,label in zip(files,labels):\n",
        "    facedetect = Face_Detection_Recognition_Video()\n",
        "    img = load_data_image(fil)\n",
        "    if not type(img) == type(None):\n",
        "     faces , pos = facedetect.get_face(img)\n",
        "    if not len(faces)== 0:\n",
        "      X.append(faces)\n",
        "      newlabels.append(label)\n",
        "\n",
        "  return asarray(X), asarray(newlabels)\n",
        "\n",
        "\n",
        "\n",
        "# paths train and validation\n",
        "train_path = '/content/photos/train/'\n",
        "val_path = '/content/photos/val/'\n",
        "# load train dataset\n",
        "trainX, trainy = faces_dataset(train_path)\n",
        "# load test dataset\n",
        "testX, testy = faces_dataset(val_path)\n",
        "# save arrays to one file in compressed format\n",
        "# savez_compressed('5-celebrity-faces-dataset.npz', trainX, trainy, testX, testy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON3lauCj1p44",
        "outputId": "5614ea26-1bb8-4f94-c811-4a0c23a5627d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(114, 160, 160, 3)\n",
            "(30, 160, 160, 3)\n",
            "(114,)\n",
            "(30,)\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for Cnn\n",
        "\n",
        "Cnn_trainx = []\n",
        "for p in trainX:\n",
        "  Cnn_trainx.append(p[0])\n",
        "Cnn_trainx = np.array(Cnn_trainx)\n",
        "print(Cnn_trainx.shape)\n",
        "\n",
        "Cnn_testx = []\n",
        "for p in testX:\n",
        "  Cnn_testx.append(p[0])\n",
        "Cnn_testx = np.array(Cnn_testx)\n",
        "print(Cnn_testx.shape)\n",
        "\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "train_y = out_encoder.transform(trainy)\n",
        "print(train_y.shape)\n",
        "test_y = out_encoder.transform(testy)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtK6RA18dRjx",
        "outputId": "d60a073e-cfda-49bd-ac2a-a83be9044a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8280 (Conv2D)        (None, 158, 158, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_4140 (MaxPoo  (None, 79, 79, 32)        0         \n",
            " ling2D)                                                         \n",
            "                                                                 \n",
            " conv2d_8281 (Conv2D)        (None, 77, 77, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_4141 (MaxPoo  (None, 38, 38, 64)        0         \n",
            " ling2D)                                                         \n",
            "                                                                 \n",
            " conv2d_8282 (Conv2D)        (None, 36, 36, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_4142 (MaxPoo  (None, 18, 18, 128)       0         \n",
            " ling2D)                                                         \n",
            "                                                                 \n",
            " flatten_1380 (Flatten)      (None, 41472)             0         \n",
            "                                                                 \n",
            " dense_4830 (Dense)          (None, 512)               21234176  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_4831 (Dense)          (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_4832 (Dense)          (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21393862 (81.61 MB)\n",
            "Trainable params: 21393862 (81.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "4/4 [==============================] - 6s 281ms/step - loss: 377.1084 - accuracy: 0.1754\n",
            "Epoch 2/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 40.2732 - accuracy: 0.2632\n",
            "Epoch 3/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 2.0635 - accuracy: 0.4649\n",
            "Epoch 4/25\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.8218 - accuracy: 0.7105\n",
            "Epoch 5/25\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3753 - accuracy: 0.8596\n",
            "Epoch 6/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3539 - accuracy: 0.8860\n",
            "Epoch 7/25\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.7945 - accuracy: 0.7632\n",
            "Epoch 8/25\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.2482 - accuracy: 0.9123\n",
            "Epoch 9/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.1071 - accuracy: 0.9737\n",
            "Epoch 10/25\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.0444 - accuracy: 0.9912\n",
            "Epoch 11/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0414 - accuracy: 0.9912\n",
            "Epoch 12/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0972 - accuracy: 0.9825\n",
            "Epoch 13/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.5386 - accuracy: 0.8246\n",
            "Epoch 14/25\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.2404 - accuracy: 0.9649\n",
            "Epoch 15/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2924 - accuracy: 0.9561\n",
            "Epoch 16/25\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.2347 - accuracy: 0.9298\n",
            "Epoch 17/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0895 - accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0518 - accuracy: 0.9912\n",
            "Epoch 19/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0604 - accuracy: 0.9912\n",
            "Epoch 20/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.0309 - accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0706 - accuracy: 0.9825\n",
            "Epoch 23/25\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.0084 - accuracy: 1.0000\n",
            "1/1 [==============================] - 1s 581ms/step - loss: 3.5908 - accuracy: 0.6333\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "face_detect = Face_Detection_Recognition_Video()\n",
        "train_model = face_detect.train_cnn_model(Cnn_trainx,train_y,Cnn_testx,test_y,25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu-21wng609A"
      },
      "outputs": [],
      "source": [
        "# recognition face in photo\n",
        "image = load_data_image('/content/photos/train/zahra_dabiri/20220609_210252.jpg')\n",
        "face_detect = Face_Detection_Recognition_Video()\n",
        "faces , pos = face_detect.get_face(image)\n",
        "# print(image.shape)\n",
        "for i in range(len(faces)):\n",
        "  recog_model = face_detect.recognition_model(faces[i],train_model)\n",
        "  pyplot.figure()\n",
        "  pyplot.imshow(image)\n",
        "  ax = pyplot.gca()\n",
        "  # get coordinates\n",
        "  x, y, width, height = pos[i]['box']\n",
        "  # create the shape\n",
        "  rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
        "  # draw the box\n",
        "  ax.add_patch(rect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyH8prR_QN9k"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image as Img\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09b_0FAnUa9y"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = Img.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghUlAJzKSjFT"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nkSnkbkk4cC"
      },
      "outputs": [],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces , pos = face_detect.get_face(img)\n",
        "    # get face bounding box for overlay\n",
        "    # for (x,y,w,h)in pos:\n",
        "    #   bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    for i in range(len(faces)):\n",
        "      recog_model = face_detect.recognition_model(faces[i],train_model)\n",
        "      txt = recog_model[0]\n",
        "      # print(txt , type(txt))\n",
        "      font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "      # fontScale\n",
        "      fontScale = 1\n",
        "      # Blue color in BGR\n",
        "      color = (255, 0, 0)\n",
        "      # Line thickness of 2 px\n",
        "      thickness = 2\n",
        "\n",
        "      x, y, w, h = pos[i]['box']\n",
        "      # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "      bbox_array = cv2.putText(bbox_array,txt,(x,y-10),font,fontScale,color,thickness)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

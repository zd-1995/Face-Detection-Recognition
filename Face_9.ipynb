{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MTCNN & CNN & colab video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ullg3XjJVgiv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1H3Zf2NpVUf"
      },
      "outputs": [],
      "source": [
        "!pip install MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EwpCJfRlocTA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import requests\n",
        "from io import BytesIO\n",
        "# extract and plot each detected face in a photograph\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle,Circle\n",
        "from mtcnn import MTCNN\n",
        "# from mtcnn.mtcnn import MTCNN\n",
        "# from google.colab.patches import cv2_imshow\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder,Normalizer\n",
        "from sklearn.svm import SVC\n",
        "from random import choice\n",
        "from PIL import Image as Img\n",
        "from numpy import savez_compressed,asarray,load,expand_dims\n",
        "from keras.models import load_model\n",
        "# from keras_facenet import FaceNet\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nx_16lnMqjvF"
      },
      "outputs": [],
      "source": [
        "# class Data_Using:\n",
        "\n",
        "def unzip(zip_dir,out_dir):\n",
        "    path_folder = out_dir\n",
        "\n",
        "    zip_file_path = f'{zip_dir}'\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(path_folder)\n",
        "\n",
        "\n",
        "def load_dataset(directory):\n",
        "  \t# enumerate folders, on per class\n",
        "  images = []\n",
        "  path_d = []\n",
        "  allpath = []\n",
        "  y = []\n",
        "  for subdir in os.listdir(directory):\n",
        "\n",
        "    # print(y)\n",
        "       \t# enumerate files\n",
        "    for filename in os.listdir(directory+subdir+'/'):\n",
        "      path = directory + subdir + '/' + filename\n",
        "          #labels\n",
        "      y.append(subdir)\n",
        "      path_d.append(path)\n",
        "      # print(path_d)\n",
        "  # allpath.extend(path_d)\n",
        "\n",
        "  print(path_d)\n",
        "\t  \t# store\n",
        "    # images.append(path_d)\n",
        "  return path_d , y\n",
        "\n",
        "\n",
        "def load_data_image(path_image):\n",
        "    img = cv2.imread(path_image)\n",
        "    # img = cv2.cvtColor(img,cv2.COLOR)\n",
        "    return img\n",
        "\n",
        "\n",
        "def load_data_video(path_video):\n",
        "\n",
        "    frame_list = []\n",
        "    # Load the video content using OpenCV\n",
        "    cap = cv2.VideoCapture(path_video)\n",
        "    # Loop through the frames and display them\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Display the frame\n",
        "        # cv2_imshow( frame)\n",
        "\n",
        "        frame_list.append(frame)\n",
        "\n",
        "        # Press 'q' to exit the loop\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    # Release the video capture object and close all windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    frame_list = np.array(frame_list)\n",
        "    return frame_list\n",
        "\n",
        "\n",
        "\n",
        "def load_data_system_camera():\n",
        "    # define a video capture object\n",
        "    vid = cv2.VideoCapture(0)\n",
        "    while(True):\n",
        "        # Capture the video frame\n",
        "        ret, frame = vid.read()\n",
        "        # Display the resulting frame\n",
        "        cv2.imshow('frame', frame)\n",
        "        # cv2.imwrite('video.mp4',frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    # After the loop release the cap object\n",
        "    vid.release()\n",
        "    # Destroy all the windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dK8LKMVIMZqv"
      },
      "outputs": [],
      "source": [
        "from functools import wraps\n",
        "import sys\n",
        "import io\n",
        "def capture_output(func):\n",
        "    \"\"\"Wrapper to capture print output.\"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        old_stdout = sys.stdout\n",
        "        new_stdout = io.StringIO()\n",
        "        sys.stdout = new_stdout\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mokJz5IlSoS6"
      },
      "outputs": [],
      "source": [
        "class Face_Detection_Recognition_Video:\n",
        "\n",
        "    def __init__(self):\n",
        "      # super.__init__(self)\n",
        "      self.face_number = 0\n",
        "      self.folder_number = 0\n",
        "      self.detector = MTCNN()\n",
        "      self.w_detect = capture_output( self.detector.detect_faces)\n",
        "\n",
        "    # def preprocess(self,image):\n",
        "    #   #normalize\n",
        "    #     pixels = image.astype('float32')\n",
        "    #     pixels /= 255.0\n",
        "    #     return pixels\n",
        "\n",
        "      # draw each face separately\n",
        "    def mtcnn_model(self,data, result_list):\n",
        "        # get the context for drawing boxes\n",
        "        # ax = pyplot.gca()\n",
        "        # for result in result_list:\n",
        "        #   # get coordinates\n",
        "        #   x, y, width, height = result['box']\n",
        "        #   # create the shape\n",
        "        #   rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
        "        #   # draw the box\n",
        "        #   ax.add_patch(rect)\n",
        "        #   # draw the dots\n",
        "        #   for key, value in result['keypoints'].items():\n",
        "        #   # create and draw dot\n",
        "        #     dot = Circle(value, radius=2, color='red')\n",
        "        #     ax.add_patch(dot)\n",
        "        # pyplot.show()\n",
        "        crop_face = []\n",
        "        for i in range(len(result_list)):\n",
        "          # get coordinates\n",
        "          x1, y1, width, height = result_list[i]['box']\n",
        "          x2, y2 = x1 + width, y1 + height\n",
        "          # define subplot\n",
        "          # pyplot.subplot(1, len(result_list), i+1)\n",
        "          # pyplot.axis('off')\n",
        "          # plot face\n",
        "          Face = data[y1:y2, x1:x2]\n",
        "          # resize pixels to the model size\n",
        "          Face = Img.fromarray(Face)\n",
        "          Face = Face.convert('RGB')\n",
        "          Face = Face.resize((160,160))\n",
        "          Face = asarray(Face)#,dtype=np.float32)\n",
        "          crop_face.append(Face)\n",
        "\n",
        "        return crop_face\n",
        "\n",
        "\n",
        "    def get_face(self,path):\n",
        "        pixels = path\n",
        "        pixels = asarray(pixels)\n",
        "        # detector = MTCNN()\n",
        "        # detect faces in the image\n",
        "        # w_detect = capture_output( self.detector.detect_faces)\n",
        "        start_1=time.time()\n",
        "        faces_pos = self.w_detect(pixels)\n",
        "        # stop=time.time()\n",
        "        start=time.time()\n",
        "        Faces = self.mtcnn_model(pixels, faces_pos)\n",
        "        stop=time.time()\n",
        "        print('detect_time:',start-start_1)\n",
        "        print('all_time:',stop-start_1)\n",
        "        # print(stop)\n",
        "\n",
        "        return Faces , faces_pos\n",
        "\n",
        "\n",
        "    def model_CNN(self):\n",
        "      model_cnn = models.Sequential([\n",
        "      layers.Conv2D(16, (3, 3), activation='relu', input_shape=(160, 160, 3)),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Conv2D(32, (3, 3), activation='relu'),# input_shape=(160, 160, 3)),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      # layers.Dropout(0.2),\n",
        "      layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      # layers.Dropout(0.2),\n",
        "      layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "      layers.MaxPooling2D((2, 2)),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(512, activation='relu'),\n",
        "      layers.Dropout(0.2),  # Dropout for regularization\n",
        "      layers.Dense(128, activation='relu'),\n",
        "      # layers.Dropout(0.2),\n",
        "      # layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(9, activation='softmax')  # num_classes is the number of identities\n",
        "      ])\n",
        "\n",
        "      return model_cnn\n",
        "\n",
        "    def train_cnn_model(self,trainx,trainy,testx, testy,E_number):\n",
        "      self.model = self.model_CNN()\n",
        "      self.model.summary()\n",
        "      self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "      self.model.fit(trainx, trainy, epochs= E_number)\n",
        "      test_loss, test_acc = self.model.evaluate(testx, testy)\n",
        "      return self.model , test_loss, test_acc\n",
        "\n",
        "\n",
        "    def recognition_model(self,image,model):\n",
        "      random_face = np.array(image)\n",
        "      random_face = random_face.reshape(1,160,160,3)\n",
        "      w_predict = capture_output(self.model.predict)\n",
        "      pred = w_predict(random_face)\n",
        "      # random_face_name = out_encoder.inverse_transform([random_face_class])\n",
        "      out_encoder = LabelEncoder()\n",
        "      out_encoder.fit(trainy)\n",
        "      class_index = tf.argmax(pred, axis=1)\n",
        "      # confidence = tf.reduce_max(pred[0][random_face_class])\n",
        "      pred_name = out_encoder.inverse_transform(class_index)\n",
        "      # print('Predicted: %s' % pred_name)\n",
        "      # print('Expected: %s' % random_face_name[0])\n",
        "      # print(f\"Predicted class: {class_index}, Confidence: {confidence * 100:.2f}%\")\n",
        "      # pyplot.figure()\n",
        "      # pyplot.imshow(random_face.reshape(160,160,3))\n",
        "      # pyplot.show()\n",
        "      return pred_name\n",
        "\n",
        "    def save_faces(self,data_face):\n",
        "      folder_name = 'Faces'+f'{self.folder_number}'\n",
        "      if not os.path.isdir(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "      cv2.imwrite(folder_name + '/' + f'{self.face_number}.jpg',data_face)\n",
        "      self.face_number += 1\n",
        "\n",
        "    def face_counting(self,):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XO6ub9w-QK8o"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Facenet/Facedata.zip'\n",
        "output = '/content/photos'\n",
        "data = unzip(path,output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tY2grl0J6dCz"
      },
      "outputs": [],
      "source": [
        "!rm -R /content/photos/__MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KT1kpS3Amx-P"
      },
      "outputs": [],
      "source": [
        "!find . -name \"*.DS_Store\" -type f -delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFYuXgmBAmG7"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "\n",
        "def faces_dataset(directory):\n",
        "  X = list()\n",
        "  newlabels = list()\n",
        "  files , labels = load_dataset(directory)\n",
        "  print(len(files),len(labels))\n",
        "  for fil,label in zip(files,labels):\n",
        "    facedetect = Face_Detection_Recognition_Video()\n",
        "    img = load_data_image(fil)\n",
        "    if not type(img) == type(None):\n",
        "     faces , pos = facedetect.get_face(img)\n",
        "    if not len(faces)== 0:\n",
        "      X.append(faces)\n",
        "      newlabels.append(label)\n",
        "\n",
        "  return asarray(X), asarray(newlabels)\n",
        "\n",
        "\n",
        "\n",
        "# paths train and validation\n",
        "train_path = '/content/photos/train/'\n",
        "val_path = '/content/photos/val/'\n",
        "# load train dataset\n",
        "trainX, trainy = faces_dataset(train_path)\n",
        "# load test dataset\n",
        "testX, testy = faces_dataset(val_path)\n",
        "# save arrays to one file in compressed format\n",
        "# savez_compressed('5-celebrity-faces-dataset.npz', trainX, trainy, testX, testy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON3lauCj1p44",
        "outputId": "7fe937b3-0beb-4cc0-9a07-7734bd397a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(230, 160, 160, 3)\n",
            "(44, 160, 160, 3)\n",
            "(230,)\n",
            "(44,)\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for Cnn\n",
        "\n",
        "Cnn_trainx = []\n",
        "for p in trainX:\n",
        "  Cnn_trainx.append(p[0])\n",
        "Cnn_trainx = np.array(Cnn_trainx)\n",
        "print(Cnn_trainx.shape)\n",
        "\n",
        "Cnn_testx = []\n",
        "for p in testX:\n",
        "  Cnn_testx.append(p[0])\n",
        "Cnn_testx = np.array(Cnn_testx)\n",
        "print(Cnn_testx.shape)\n",
        "\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "train_y = out_encoder.transform(trainy)\n",
        "print(train_y.shape)\n",
        "test_y = out_encoder.transform(testy)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtK6RA18dRjx",
        "outputId": "34c8e755-e1e3-409f-ba96-61611953aabd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3468 (Conv2D)        (None, 158, 158, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d_1734 (MaxPool  (None, 79, 79, 16)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_3469 (Conv2D)        (None, 77, 77, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1735 (MaxPool  (None, 38, 38, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_3470 (Conv2D)        (None, 36, 36, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1736 (MaxPool  (None, 18, 18, 64)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_3471 (Conv2D)        (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_1737 (MaxPool  (None, 8, 8, 128)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " flatten_578 (Flatten)       (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_2023 (Dense)          (None, 512)               4194816   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2024 (Dense)          (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_2025 (Dense)          (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,359,081\n",
            "Trainable params: 4,359,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 11s 1s/step - loss: 42.7961 - accuracy: 0.1261\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 6s 735ms/step - loss: 2.4098 - accuracy: 0.1783\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 7s 913ms/step - loss: 1.9209 - accuracy: 0.2870\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 7s 823ms/step - loss: 1.6654 - accuracy: 0.4913\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 6s 757ms/step - loss: 1.1712 - accuracy: 0.6261\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 8s 989ms/step - loss: 0.7100 - accuracy: 0.7739\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 6s 734ms/step - loss: 0.5225 - accuracy: 0.8391\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2862 - accuracy: 0.8739\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 6s 745ms/step - loss: 0.2438 - accuracy: 0.9217\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.2938 - accuracy: 0.9304\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 6s 738ms/step - loss: 0.1788 - accuracy: 0.9652\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 7s 867ms/step - loss: 0.1169 - accuracy: 0.9783\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 8s 891ms/step - loss: 0.0653 - accuracy: 0.9870\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 6s 748ms/step - loss: 0.1130 - accuracy: 0.9696\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0815 - accuracy: 0.9826\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 6s 753ms/step - loss: 0.1135 - accuracy: 0.9739\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0966 - accuracy: 0.9913\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 6s 750ms/step - loss: 0.0468 - accuracy: 0.9870\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 8s 985ms/step - loss: 0.0245 - accuracy: 0.9913\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 7s 774ms/step - loss: 0.1814 - accuracy: 0.9565\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 7s 815ms/step - loss: 0.1055 - accuracy: 0.9696\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 8s 971ms/step - loss: 0.1236 - accuracy: 0.9652\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 6s 773ms/step - loss: 0.0557 - accuracy: 0.9913\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.1375 - accuracy: 0.9652\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 6s 741ms/step - loss: 0.0945 - accuracy: 0.9696\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0554 - accuracy: 0.9870\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 6s 739ms/step - loss: 0.0136 - accuracy: 0.9957\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 7s 922ms/step - loss: 0.0107 - accuracy: 0.9957\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 7s 813ms/step - loss: 0.0170 - accuracy: 0.9957\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 6s 753ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0419 - accuracy: 0.9957\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 6s 722ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0188 - accuracy: 0.9957\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 6s 732ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 8s 992ms/step - loss: 0.0204 - accuracy: 0.9957\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 7s 750ms/step - loss: 0.0158 - accuracy: 0.9957\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 6s 807ms/step - loss: 0.0416 - accuracy: 0.9957\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 8s 937ms/step - loss: 0.0198 - accuracy: 0.9957\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 6s 742ms/step - loss: 0.0206 - accuracy: 0.9913\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0429 - accuracy: 0.9913\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 6s 732ms/step - loss: 0.0153 - accuracy: 0.9913\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0206 - accuracy: 0.9913\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 6s 733ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 7s 875ms/step - loss: 0.0120 - accuracy: 0.9957\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 7s 862ms/step - loss: 0.0054 - accuracy: 0.9957\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 6s 723ms/step - loss: 0.0167 - accuracy: 0.9957\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0149 - accuracy: 0.9957\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 6s 735ms/step - loss: 0.0368 - accuracy: 0.9957\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0770 - accuracy: 0.9913\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 6s 743ms/step - loss: 0.0199 - accuracy: 0.9913\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 8s 970ms/step - loss: 0.0518 - accuracy: 0.9826\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 7s 781ms/step - loss: 0.3030 - accuracy: 0.9304\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 6s 788ms/step - loss: 0.8082 - accuracy: 0.7913\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 8s 947ms/step - loss: 0.5650 - accuracy: 0.8565\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 6s 740ms/step - loss: 2.9116 - accuracy: 0.3870\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 2.4301 - accuracy: 0.4174\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 6s 728ms/step - loss: 1.4038 - accuracy: 0.5174\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9761 - accuracy: 0.6783\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 6s 739ms/step - loss: 0.7222 - accuracy: 0.7826\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 7s 886ms/step - loss: 0.7497 - accuracy: 0.8043\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 8s 858ms/step - loss: 0.4615 - accuracy: 0.8435\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4671 - accuracy: 0.8696\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 13s 1s/step - loss: 0.2186 - accuracy: 0.9217\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 6s 745ms/step - loss: 0.1153 - accuracy: 0.9739\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0969 - accuracy: 0.9739\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 6s 764ms/step - loss: 0.1028 - accuracy: 0.9696\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0623 - accuracy: 0.9696\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 6s 744ms/step - loss: 0.0649 - accuracy: 0.9870\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 8s 984ms/step - loss: 0.0554 - accuracy: 0.9826\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 7s 775ms/step - loss: 0.2113 - accuracy: 0.9522\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 6s 800ms/step - loss: 0.0611 - accuracy: 0.9870\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 8s 957ms/step - loss: 0.0369 - accuracy: 0.9913\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 6s 731ms/step - loss: 0.1459 - accuracy: 0.9783\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0854 - accuracy: 0.9826\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 6s 743ms/step - loss: 0.0600 - accuracy: 0.9826\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0197 - accuracy: 0.9957\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 6s 715ms/step - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 6s 808ms/step - loss: 0.0398 - accuracy: 0.9957\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 8s 918ms/step - loss: 0.0210 - accuracy: 0.9957\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 6s 707ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0183 - accuracy: 0.9957\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 6s 719ms/step - loss: 0.0144 - accuracy: 0.9913\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 6s 731ms/step - loss: 0.0201 - accuracy: 0.9957\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 7s 832ms/step - loss: 0.0453 - accuracy: 0.9913\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 8s 921ms/step - loss: 0.0050 - accuracy: 0.9957\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 6s 730ms/step - loss: 0.0085 - accuracy: 0.9957\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0197 - accuracy: 0.9957\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 6s 732ms/step - loss: 0.0540 - accuracy: 0.9870\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0969 - accuracy: 0.9739\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 6s 729ms/step - loss: 0.0244 - accuracy: 0.9913\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 7s 923ms/step - loss: 0.0749 - accuracy: 0.9739\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 7s 805ms/step - loss: 0.0609 - accuracy: 0.9826\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 6s 733ms/step - loss: 0.0753 - accuracy: 0.9826\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 8s 998ms/step - loss: 0.2787 - accuracy: 0.9478\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 6s 731ms/step - loss: 0.1298 - accuracy: 0.9435\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0814 - accuracy: 0.9739\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 6s 729ms/step - loss: 0.0895 - accuracy: 0.9870\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 8s 998ms/step - loss: 0.0271 - accuracy: 0.9913\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.0152 - accuracy: 0.9913\n",
            "2/2 [==============================] - 1s 87ms/step - loss: 3.4693 - accuracy: 0.5455\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "face_detect = Face_Detection_Recognition_Video()\n",
        "train_model = face_detect.train_cnn_model(Cnn_trainx,train_y,Cnn_testx,test_y,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDWQ2drkGZV_"
      },
      "outputs": [],
      "source": [
        "train_model[0].save_weights('/content/drive/MyDrive/Facenet/Model/weightmodel')\n",
        "train_model[0].save('/content/drive/MyDrive/Facenet/Model/Mymodel.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu-21wng609A"
      },
      "outputs": [],
      "source": [
        "# recognition face in photo\n",
        "image = load_data_image('/content/photos/train/zahra_dabiri/20220609_210252.jpg')\n",
        "face_detect = Face_Detection_Recognition_Video()\n",
        "faces , pos = face_detect.get_face(image)\n",
        "# print(image.shape)\n",
        "for i in range(len(faces)):\n",
        "  recog_model = face_detect.recognition_model(faces[i],train_model)\n",
        "  pyplot.figure()\n",
        "  pyplot.imshow(image)\n",
        "  ax = pyplot.gca()\n",
        "  # get coordinates\n",
        "  x, y, width, height = pos[i]['box']\n",
        "  # create the shape\n",
        "  rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
        "  # draw the box\n",
        "  ax.add_patch(rect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tyH8prR_QN9k"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image as Img\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "09b_0FAnUa9y"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = Img.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ghUlAJzKSjFT"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "1nkSnkbkk4cC",
        "outputId": "b7c32fc3-6b37-42f4-cd9a-3ad2770f7008"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    var video;\n    var div = null;\n    var stream;\n    var captureCanvas;\n    var imgElement;\n    var labelElement;\n\n    var pendingResolve = null;\n    var shutdown = false;\n\n    function removeDom() {\n       stream.getVideoTracks()[0].stop();\n       video.remove();\n       div.remove();\n       video = null;\n       div = null;\n       stream = null;\n       imgElement = null;\n       captureCanvas = null;\n       labelElement = null;\n    }\n\n    function onAnimationFrame() {\n      if (!shutdown) {\n        window.requestAnimationFrame(onAnimationFrame);\n      }\n      if (pendingResolve) {\n        var result = \"\";\n        if (!shutdown) {\n          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n        }\n        var lp = pendingResolve;\n        pendingResolve = null;\n        lp(result);\n      }\n    }\n\n    async function createDom() {\n      if (div !== null) {\n        return stream;\n      }\n\n      div = document.createElement('div');\n      div.style.border = '2px solid black';\n      div.style.padding = '3px';\n      div.style.width = '100%';\n      div.style.maxWidth = '600px';\n      document.body.appendChild(div);\n\n      const modelOut = document.createElement('div');\n      modelOut.innerHTML = \"<span>Status:</span>\";\n      labelElement = document.createElement('span');\n      labelElement.innerText = 'No data';\n      labelElement.style.fontWeight = 'bold';\n      modelOut.appendChild(labelElement);\n      div.appendChild(modelOut);\n\n      video = document.createElement('video');\n      video.style.display = 'block';\n      video.width = div.clientWidth - 6;\n      video.setAttribute('playsinline', '');\n      video.onclick = () => { shutdown = true; };\n      stream = await navigator.mediaDevices.getUserMedia(\n          {video: { facingMode: \"environment\"}});\n      div.appendChild(video);\n\n      imgElement = document.createElement('img');\n      imgElement.style.position = 'absolute';\n      imgElement.style.zIndex = 1;\n      imgElement.onclick = () => { shutdown = true; };\n      div.appendChild(imgElement);\n\n      const instruction = document.createElement('div');\n      instruction.innerHTML =\n          '<span style=\"color: red; font-weight: bold;\">' +\n          'When finished, click here or on the video to stop this demo</span>';\n      div.appendChild(instruction);\n      instruction.onclick = () => { shutdown = true; };\n\n      video.srcObject = stream;\n      await video.play();\n\n      captureCanvas = document.createElement('canvas');\n      captureCanvas.width = 640; //video.videoWidth;\n      captureCanvas.height = 480; //video.videoHeight;\n      window.requestAnimationFrame(onAnimationFrame);\n\n      return stream;\n    }\n    async function stream_frame(label, imgData) {\n      if (shutdown) {\n        removeDom();\n        shutdown = false;\n        return '';\n      }\n\n      var preCreate = Date.now();\n      stream = await createDom();\n\n      var preShow = Date.now();\n      if (label != \"\") {\n        labelElement.innerHTML = label;\n      }\n\n      if (imgData != \"\") {\n        var videoRect = video.getClientRects()[0];\n        imgElement.style.top = videoRect.top + \"px\";\n        imgElement.style.left = videoRect.left + \"px\";\n        imgElement.style.width = videoRect.width + \"px\";\n        imgElement.style.height = videoRect.height + \"px\";\n        imgElement.src = imgData;\n      }\n\n      var preCapture = Date.now();\n      var result = await new Promise(function(resolve, reject) {\n        pendingResolve = resolve;\n      });\n      shutdown = false;\n\n      return {'create': preShow - preCreate,\n              'show': preCapture - preShow,\n              'capture': Date.now() - preCapture,\n              'img': result};\n    }\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "detect_time: 1.288156270980835\n",
            "all_time: 1.2881629467010498\n",
            "detect_time: 1.7811272144317627\n",
            "all_time: 1.783987045288086\n",
            "detect_time: 0.940363883972168\n",
            "all_time: 0.9420263767242432\n",
            "detect_time: 0.9572761058807373\n",
            "all_time: 0.9590651988983154\n",
            "detect_time: 0.9216337203979492\n",
            "all_time: 0.9234073162078857\n",
            "detect_time: 0.9193572998046875\n",
            "all_time: 0.9209990501403809\n",
            "detect_time: 1.4058005809783936\n",
            "all_time: 1.4087865352630615\n",
            "detect_time: 0.9195396900177002\n",
            "all_time: 0.9210314750671387\n",
            "detect_time: 0.9212265014648438\n",
            "all_time: 0.9227612018585205\n",
            "detect_time: 0.9142789840698242\n",
            "all_time: 0.915369987487793\n",
            "detect_time: 1.3764195442199707\n",
            "all_time: 1.3782432079315186\n"
          ]
        }
      ],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces , pos = face_detect.get_face(img)\n",
        "    # get face bounding box for overlay\n",
        "    # for (x,y,w,h)in pos:\n",
        "    #   bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    for i in range(len(faces)):\n",
        "      recog_model = face_detect.recognition_model(faces[i],train_model)\n",
        "      txt = recog_model[0]\n",
        "      # print(txt , type(txt))\n",
        "      font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "      # fontScale\n",
        "      fontScale = 1\n",
        "      # Blue color in BGR\n",
        "      color = (255, 0, 0)\n",
        "      # Line thickness of 2 px\n",
        "      thickness = 2\n",
        "\n",
        "      x, y, w, h = pos[i]['box']\n",
        "      # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "      bbox_array = cv2.putText(bbox_array,txt,(x,y-10),font,fontScale,color,thickness)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
